{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글-Claude-v2 Model: Retrieval Augmented Question & Answering with Amazon Bedrock using LangChain\n",
    "\n",
    "---\n",
    "### 중요\n",
    "- 이 노트북은 Anthropic 의 Claude-v2 모델 접근 가능한 분만 실행 가능합니다. \n",
    "- 접근이 안되시는 분은 노트북의 코드와 결과 만을 확인 하시면 좋겠습니다.\n",
    "- 만일 실행시에는 **\"과금\"** 이 발생이 되는 부분 유념 해주시기 바랍니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 기본 지식\n",
    "\n",
    "### 문맥\n",
    "이전에는 모델이 타이어 교체 방법을 알려주는 것을 보았지만 관련 데이터를 수동으로 제공하고 컨텍스트를 직접 제공해야 했습니다. 우리는 Bedrock에서 사용 가능한 모델을 활용하고 교육 중에 배운 지식을 바탕으로 질문하고 수동 컨텍스트를 제공하는 접근 방식을 탐색했습니다. 이러한 접근 방식은 짧은 문서나 단일 톤 애플리케이션에서는 작동하지만, 모델에 전송된 프롬프트에 모두 들어맞을 수 없는 대규모 기업 문서가 있을 수 있는 기업 수준의 질문 응답으로 확장되지 않습니다.\n",
    "\n",
    "### 패턴\n",
    "RAG(Retreival Augmented Generation)라는 아키텍처를 구현하여 이 프로세스를 개선할 수 있습니다. RAG는 ​​언어 모델 외부(비모수적)에서 데이터를 검색하고 검색된 관련 데이터를 컨텍스트에 추가하여 프롬프트를 강화합니다.\n",
    "\n",
    "이 노트에서는 질문 응답 패턴에 접근하여 문서를 찾고 활용하여 사용자 질문에 대한 답변을 제공하는 방법을 설명합니다.\n",
    "\n",
    "### 챌린지\n",
    "- 토큰 한도를 초과하는 대용량 문서 관리 방법\n",
    "- 질문과 관련된 문서를 찾는 방법\n",
    "\n",
    "### 제안\n",
    "위의 과제에 대해 이 노트는 다음과 같은 전략을 제안합니다.\n",
    "#### 도큐먼트 준비\n",
    "![임베딩](./images/Embeddings_lang.png)\n",
    "\n",
    "질문에 답하려면 먼저 문서를 처리하고 문서 저장소 인덱스에 저장해야 합니다.\n",
    "- 문서를 로드\n",
    "- 더 작은 덩어리(chunk)로 처리하고 분할합니다.\n",
    "- Amazon Bedrock Titan Embeddings 모델을 사용하여 각 청크의 수치 벡터 표현 생성\n",
    "- 청크와 해당 임베딩을 사용하여 인덱스 생성\n",
    "#### Ask Question\n",
    "![질문](./images/Chatbot_lang.png)\n",
    "\n",
    "문서 색인이 준비되면 질문할 준비가 된 것이며, 질문한 질문에 따라 관련 문서를 가져옵니다. 다음 단계가 실행됩니다.\n",
    "- 입력 질문의 임베딩 생성\n",
    "- 질문 임베딩과 인덱스의 임베딩을 비교합니다.\n",
    "- (상위 N) 관련 문서 청크를 가져옵니다.\n",
    "- 프롬프트에서 컨텍스트의 일부로 해당 청크를 추가합니다.\n",
    "- Amazon Bedrock의 모델에 프롬프트를 보냅니다.\n",
    "- 검색된 문서를 기반으로 상황에 맞는 답변을 얻습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 사례\n",
    "#### 데이터세트\n",
    "이 아키텍처 패턴을 설명하기 위해 \"교보생명 건강보험 약관 문서를 사용합니다. 이 문서에서는 다음과 같은 주제를 설명합니다.\n",
    "- 보험 보장 내용\n",
    "- 보험 가입 안내\n",
    "- 보험료 및 해지 환급금\n",
    "\n",
    "#### 페르소나\n",
    "보험 설계사가 질문에 답변을 제공하듯이 보험 내용을 상세히 이해하지 못하는 일반인의 페르소나를 가정해 보겠습니다.\n",
    "모델은 문서를 바탕으로 쉬운 언어로 답변하려고 노력할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구현\n",
    "RAG 접근 방식을 따르기 위해 이 노트북은 RAG와 같은 패턴을 효율적으로 구축할 수 있는 다양한 서비스 및 도구와 통합된 LangChain 프레임워크를 사용하고 있습니다. 우리는 다음 도구를 사용할 것입니다:\n",
    "\n",
    "- **LLM(대형 언어 모델)**: Amazon Bedrock을 통해 사용 가능한 Anthropic Claude V2\n",
    "\n",
    "  이 모델은 문서 덩어리(chunk)를 이해하고 인간 친화적인 방식으로 답변을 제공하는 데 사용됩니다.\n",
    "- **임베딩 모델**: Amazon Bedrock을 통해 사용 가능한 Amazon Titan 임베딩\n",
    "\n",
    "  이 모델은 텍스트 문서의 수치 표현을 생성하는 데 사용됩니다.\n",
    "- **문서 로더**: LangChain을 통해 사용 가능한 PDF 로더\n",
    "\n",
    "  이는 소스에서 문서를 로드할 수 있는 로더입니다. 이 노트북을 위해 로컬 경로에서 샘플 파일을 로드합니다. 이는 기업 내부 시스템에서 문서를 로드하는 로더로 쉽게 대체될 수 있습니다.\n",
    "\n",
    "- **Vector Store**: LangChain을 통해 FAISS 사용\n",
    "\n",
    "  이 노트북에서는 이 메모리 내 벡터 저장소를 사용하여 임베딩과 문서를 모두 저장합니다. 기업 환경에서는 이는 AWS OpenSearch, pgVector가 포함된 RDS Postgres, ChromaDB, Pinecone 또는 Weaviate와 같은 영구 저장소로 대체될 수 있습니다.\n",
    "- **색인**: 벡터인덱스\n",
    "\n",
    "  인덱스는 입력 임베딩과 문서 임베딩을 비교하여 관련 문서를 찾는 데 도움이 됩니다.\n",
    "- **래퍼**: 인덱스, 벡터 저장소, 임베딩 모델 및 LLM을 래핑하여 사용자의 논리를 추상화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설정\n",
    "\n",
    "이 노트북의 나머지 부분을 실행하기 전에 아래 셀을 실행하여 (필요한 라이브러리가 설치되어 있는지 확인하고) Bedrock에 연결해야 합니다.\n",
    "\n",
    "이 노트북에는 몇 가지 추가 종속성도 필요합니다.\n",
    "\n",
    "- [FAISS](https://github.com/facebookresearch/faiss), 벡터 임베딩 저장\n",
    "- [PyPDF](https://pypi.org/project/pypdf/), PDF 파일 처리용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Bedrock Client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                            0.0.312\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mopensearch-py                        2.3.2\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip list | grep langchain\n",
    "! pip list | grep opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet \"faiss-cpu>=1.7,<2\" \"ipywidgets>=7,<8\" \"pypdf>=3.8,<4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using profile: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n",
      "\u001b[32m\n",
      "== FM lists ==\u001b[0m\n",
      "{'Claude-Instant-V1': 'anthropic.claude-instant-v1',\n",
      " 'Claude-V1': 'anthropic.claude-v1',\n",
      " 'Claude-V2': 'anthropic.claude-v2',\n",
      " 'Command': 'cohere.command-text-v14',\n",
      " 'Jurassic-2-Mid': 'ai21.j2-mid-v1',\n",
      " 'Jurassic-2-Ultra': 'ai21.j2-ultra-v1',\n",
      " 'Titan-Embeddings-G1': 'amazon.titan-embed-text-v1',\n",
      " 'Titan-Text-G1': 'TBD'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock, print_ww\n",
    "from utils.bedrock import bedrock_info\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "# os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")\n",
    "\n",
    "print(colored(\"\\n== FM lists ==\", \"green\"))\n",
    "pprint(bedrock_info.get_list_fm_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랭체인 구성\n",
    "\n",
    "LLM과 Embeddings 모델을 인스턴스화하는 것부터 시작합니다. 여기서는 텍스트 생성을 위해 Anthropic Claude를 사용하고 텍스트 삽입을 위해 Amazon Titan을 사용합니다.\n",
    "\n",
    "참고: Bedrock과 함께 사용 가능한 다른 모델을 선택할 수도 있습니다. 모델을 변경하려면 `model_id`를 다음과 같이 교체하면 됩니다.\n",
    "\n",
    "`llm = Bedrock(model_id=\"amazon.titan-tg1-large\")`\n",
    "\n",
    "사용 가능한 모델 ID는 다음과 같습니다.\n",
    "\n",
    "- amazon.titan-tg1-large\n",
    "- ai21.j2-grande-instruct\n",
    "- ai21.j2-jumbo-instruct\n",
    "- anthropic.claude-instant-v1\n",
    "- anthropic.claude-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Titan Embedding 및 LLM 인 Claude-v2 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# - create the Anthropic Model\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, \n",
    "              model_kwargs={'max_tokens_to_sample':1000})\n",
    "bedrock_embeddings = BedrockEmbeddings(client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bedrock(client=<botocore.client.BedrockRuntime object at 0x7f421e8c59f0>, model_id='anthropic.claude-v2', model_kwargs={'max_tokens_to_sample': 1000}),\n",
       " BedrockEmbeddings(client=<botocore.client.BedrockRuntime object at 0x7f421e8c59f0>, region_name=None, credentials_profile_name=None, model_id='amazon.titan-embed-text-v1', model_kwargs=None, endpoint_url=None))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm, bedrock_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 데이터 준비\n",
    "먼저 문서 저장소를 구축하기 위해 이미 제공된 \"교보 생명 건강보험 약관\" 문서를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangChain에서 사용 가능한 PyPDF의 DirectoryLoader](https://python.langchain.com/en/latest/reference/modules/document_loaders.html)를 사용하여 문서를 로드하고 더 작은 덩어리(chunk)로 나눌 수 있습니다.\n",
    "\n",
    "참고: 검색된 문서/텍스트는 질문에 대답하기에 충분한 정보를 포함할 만큼 커야 합니다. 하지만 LLM 프롬프트에 들어갈 만큼 충분히 작습니다. 또한 임베딩 모델에는 입력 토큰 길이가 512개 토큰으로 제한되어 있으며 이는 한국어의 경우에 대략 180 ~ 200 글자로 변환됩니다. 이 사용 사례를 위해 [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)를 사용하여 210자가 겹치는 약 50자의 청크를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data_kr/\")\n",
    "\n",
    "documents = loader.load()\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 210,\n",
    "    chunk_overlap  = 50,\n",
    "#    separators = ['\\n']\n",
    "     # separators = ['\\n','\\n\\n']    \n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 9 documents loaded is 1184 characters.\n",
      "After the split we have 66 documents more than the original 9.\n",
      "Average length among 66 documents (after split) is 176 characters.\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs[0].page_content: \n",
      " 무배당  교보다이렉트\n",
      "플러스건강보험  갱신형\n",
      "KYOBO LIFE INSURANCE\n",
      "이 상품안내장은 2021년 10월 기준으로 작성되었습니다.21.10 월\n",
      "상품개정\n"
     ]
    }
   ],
   "source": [
    "print(\"docs[0].page_content: \\n\", docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embedding of a document chunk:  [ 0.71484375 -0.43164062 -0.31445312 ... -0.16503906 -0.26953125\n",
      " -0.89453125]\n",
      "Size of the embedding:  (1536,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유사한 패턴에 따라 전체 코퍼스에 대해 임베딩을 생성하고 벡터 저장소에 저장할 수 있습니다.\n",
    "\n",
    "이는 임베딩 모델과 문서를 입력받아 전체 벡터 저장소를 생성하는 LangChain 내부의 [FAISS](https://github.com/facebookresearch/faiss)  구현을 사용하여 쉽게 수행할 수 있습니다. [LangChain](https://python.langchain.com/en/latest/modules/indexes/vectorstores)를 사용하면 프롬프트 생성, 쿼리 임베딩 가져오기, 관련 문서 샘플링 및 LLM 호출과 같은 대부분의 무거운 작업을 추상화할 수 있습니다. [VectorStoreIndexWrapper](https://python.langchain.com/docs/modules/data_connection/retrievers)가 이를 도와줍니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.FAISS 벡터 스토어 생성\n",
    "\n",
    "**⚠️⚠️⚠️ 참고: 다음 셀을 실행하는 데 몇 분 정도 걸릴 수 있습니다 ⚠️⚠️⚠️**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 112 ms, sys: 4.03 ms, total: 116 ms\n",
      "Wall time: 6.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreIndexWrapper(vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7f421c0a7bb0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_store_faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.질문 및 답변\n",
    "\n",
    "이제 벡터 저장소가 준비되었으므로 질문을 시작할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\\n\\nHuman:만기지급금만기 생존시 얼마 받아요?\\n\\nAssistant:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "첫 번째 단계는 문서와 비교할 수 있도록 쿼리 임베딩을 만드는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23925781, -0.15722656, -0.11376953, ...,  0.04931641,\n",
       "       -0.04077148,  0.23828125])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = bedrock_embeddings.embed_query(query)\n",
    "np.array(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 쿼리 임베딩을 사용하여 관련 문서를 가져올 수 있습니다.\n",
    "이제 쿼리는 임베딩으로 표시되어 가장 관련성이 높은 정보를 제공하는 데이터 저장소에 대해 쿼리의 유사성 검색을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents are fetched which are relevant to the query.\n",
      "----\n",
      "## Document 1: 수 있습니다.\n",
      "• 고의로 인한 사고 등 약관상 일반적으로 보장하지 않는 사항 및 위험직종 등 가입이 거절되거나 제한될 수 있는 사항에\n",
      "관하여 약관을 읽어보시기 바랍니다.\n",
      "•보험은 은행상품과 달리 위험을 보장해 드리는 상품이므로 해지환급금이 납입하신 보험료보다 적거나 없을 수 있습니다.\n",
      "•이 상품은 배당이 없는 무배당 상품입니다.\n",
      "---\n",
      "## Document 2: 맞춤형 보장이 가능합니다.\n",
      "만기에 자금마련(최초계약)\n",
      "주계약 최초계약 만기시에는 만기지급금으로\n",
      "계획적인 자금활용이 가능합니다.\n",
      "※ 갱신계약은 순수보장형만 가능함\n",
      "주계약 보험료 납입면제\n",
      "주계약의 경우 50% 이상 장해상태 또는 5대질병(뇌출혈, 급성심근경색증,\n",
      "말기신부전증, 말기간질환, 말기폐질환) 진단시 주계약 보험료 납입기간 동안\n",
      "---\n",
      "## Document 3: 금융사기 피해를 입지 않도록 유의하시기 바랍니다. 금융사기 피해관련 사항은 경찰청(112), 금융감독원(1332)으로\n",
      "    즉시 신고하시기 바랍니다.\n",
      "• 법령 및 내부통제기준에 따른 광고관련절차를 준수하였습니다.\n",
      "---\n",
      "## Document 4: 이미 납입한 보험료는 포함하지 않음) 중 큰 금액을 진단보험금으로 지급합니다.5대질병 집중보장\n",
      "뇌출혈, 급성심근경색증, 말기신부전증, 말기간질환, 말기폐질환 진단시\n",
      "안심하고 치료받을 수 있도록 일시금과 매월 생활자금(3년)을 보장해 드립니다.\n",
      "맞춤형 종합보장\n",
      "생활보장특약 3개플랜(암/간병/상해플랜) 외 각종 입원 및 수술보장 등\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "relevant_documents = vectorstore_faiss.similarity_search_by_vector(query_embedding)\n",
    "print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "print('----')\n",
    "for i, rel_doc in enumerate(relevant_documents):\n",
    "    print_ww(f'## Document {i+1}: {rel_doc.page_content}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 관련 문서가 준비되었으므로 LLM을 사용하여 이러한 문서를 기반으로 답변을 생성할 차례입니다.\n",
    "\n",
    "유사성 검색 결과를 기반으로 검색된 관련 문서와 함께 초기 프롬프트를 사용합니다. 그런 다음 이를 결합하여 모델에 피드백하여 결과를 얻는 프롬프트를 생성합니다.\n",
    "\n",
    "LangChain은 이를 쉽게 수행할 수 있는 방법에 대한 추상화를 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 빠른 방법\n",
    "Vector Store를 둘러싸서 LLM 입력을 받는 LangChain에서 제공하는 래퍼를 사용할 수 있습니다.\n",
    "이 래퍼는 뒤에서 다음 단계를 수행합니다.\n",
    "- 질문을 입력합니다.\n",
    "- 질문 임베딩 생성\n",
    "- 관련 문서 가져오기\n",
    "- 프롬프트에 문서와 질문을 채워 넣습니다.\n",
    "- 프롬프트로 모델을 호출하고 사람이 읽을 수 있는 방식으로 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\nHuman:만기지급금만기 생존시 얼마 받아요?\\n\\nAssistant:',\n",
       " Bedrock(client=<botocore.client.BedrockRuntime object at 0x7f421e8c59f0>, model_id='anthropic.claude-v2', model_kwargs={'max_tokens_to_sample': 1000}))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query, llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "주계약인 최초계약의 경우 만기에 생존시 계약시 약정한 만기지급금을 받을 수 있습니다. 만기지급금의 금액은 계약 시 설정한 보험가입금액의 범위 내에서 고객이 선택할 수 있습니다.\n",
      "\n",
      "갱신계약의 경우에는 순수보장성 상품이므로 만기시 지급금은 없습니다.\n",
      "\n",
      "정확한 만기지급금 금액은 귀하의 구체적인 계약내역을 확인해야 알 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "answer = wrapper_store_faiss.query(question=query, llm=llm)\n",
    "print_ww(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 질문을 해보죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_2 = \"\\n\\nHuman:보험료 납입면제 관련 사항 알려 주세요\\n\\nAssistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 설명된 상황에서 주요하게 알아둘 내용은 다음과 같습니다:\n",
      "\n",
      "- 보험료 납입면제는 보험료 납입기간 중에 피보험자가 사고나 질병으로 일정기간 동안 보험료를 납입하기 어려운 상황이 발생했을 때, 보험회사가 보험료 납입을 면제해주는 제도입니다.\n",
      "\n",
      "- 보험료 납입면제 사유가 발생하면 차기 보험료부터 납입이 면제됩니다.\n",
      "\n",
      "- 보험료 납입면제 기간 동안에는 보험료가 납입된 것으로 간주되어 보험혜택을 계속 받을 수 있습니다.\n",
      "\n",
      "- 보험료 납입면제 기간이 끝나면 보험료를 다시 납입해야 합니다.\n",
      "\n",
      "- 보험료 납입면제 기간 중에 보험계약을 해지하면 해지환급금이 지급됩니다.\n",
      "\n",
      "이상의 내용을 통해 보험료 납입면제 제도에 대한 주요 사항을 이해할 수 있습니다. 구체적인 궁금증이 있다면 보험사에 문의하는 것이 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "answer_2 = wrapper_store_faiss.query(question=query_2, llm=llm)\n",
    "print_ww(answer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 보험 약관에 없는 내용 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "제공된 정보만으로는 이 보험 상품의 구체적인 치과 보장 내역을 확인할 수 없습니다. 보험상품 안내문에 치과 관련 보장 내용이 구체적으로 언급되어 있지 않습니다. 치과 보장이 있는지\n",
      "여부와 그 내역은 보험상품의 상세한 약관을 확인하셔야 합니다. 제공된 정보로는 치과 보장과 관련된 구체적인 답변을 드리기 어렵습니다. 죄송합니다.\n"
     ]
    }
   ],
   "source": [
    "query_3 = \"\\n\\nHuman:치과 보장 내역 알려 주세요?\\n\\nAssistant:\"\n",
    "answer_3 = wrapper_store_faiss.query(question=query_3, llm=llm)\n",
    "print_ww(answer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.사용자 정의 가능한 옵션\n",
    "위 시나리오에서는 질문에 대한 상황 인식 답변을 빠르고 쉽게 얻을 수 있는 방법을 탐색했습니다. 이제 문서를 가져오는 방법을 사용자 정의할 수 있는 [RetrievalQA](https://python.langchain.com/en/latest/modules/chains/index_examples/Vector_db_qa.html)의 도움으로 더 사용자 정의 가능한 옵션을 살펴보겠습니다. `chain_type` 매개변수를 사용하여 프롬프트에 추가해야 합니다. 또한 검색해야 하는 관련 문서 수를 제어하려면 아래 셀에서 'k' 매개변수를 변경하여 다른 출력을 확인하세요. 많은 시나리오에서 LLM이 답변을 생성하는 데 사용한 소스 문서가 무엇인지 알고 싶을 수 있습니다. LLM 프롬프트의 컨텍스트에 추가된 문서를 반환하는 `return_source_documents`를 사용하여 출력에서 ​​해당 문서를 가져올 수 있습니다. 'RetrievalQA'를 사용하면 모델에 특정한 사용자 정의 [프롬프트 템플릿](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html)을 제공할 수도 있습니다.\n",
    "\n",
    "참고: 이 예에서는 Amazon Bedrock에서 LLM으로 Anthropic Claude를 사용하고 있습니다. 이 특정 모델은 입력이 'Human:' 아래에 제공되고 모델이 'Assistant:' 다음에 출력을 생성하도록 요청되는 경우 가장 잘 수행됩니다. 아래 셀에는 LLM이 기본 상태를 유지하고 컨텍스트 외부에서 응답하지 않도록 프롬프트를 제어하는 ​​방법의 예가 나와 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 말기폐질환, 말기간질환, 말기신부전증, 급성심근경색증, 뇌출혈 진단시 각각\n",
      "\n",
      "일시금 1,500만원 + 월생활자금 50만원 x 36개월 = 총 1,800만원이 지급됩니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\\n\\nHuman: \n",
    "다음 문맥을 사용하여 마지막 질문에 대한 간결한 답변을 제공하세요. 답을 모르면 모른다고 말하고 답을 만들어내려고 하지 마세요.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 10}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "# query = \"3년 경과 후에 해지환급금 얼마 받아요?\"  \n",
    "query = \"\\n\\nHuman:5대 질병 진단시 지급되는 금액은 얼마인가요?\\n\\nAssistant:\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "1. context to be fed into FM(e.g. Claude-v2)\n",
      "-----------------------------------------------\n",
      "이미 납입한 보험료는 포함하지 않음) 중 큰 금액을 진단보험금으로 지급합니다.5대질병 집중보장\n",
      "뇌출혈, 급성심근경색증, 말기신부전증, 말기간질환, 말기폐질환 진단시\n",
      "안심하고 치료받을 수 있도록 일시금과 매월 생활자금(3년)을 보장해 드립니다.\n",
      "맞춤형 종합보장\n",
      "생활보장특약 3개플랜(암/간병/상해플랜) 외 각종 입원 및 수술보장 등\n",
      "-----------------------------------------------\n",
      "2. context to be fed into FM(e.g. Claude-v2)\n",
      "-----------------------------------------------\n",
      "말기폐질환으로 진단확정시\n",
      "※ 다만, 최초 1회의 진단확정에 한함\n",
      "※ 갱신계약의 경우 [말기폐질환 보장계약]\n",
      "     으로 갱신된 경우에 한함일시금  1,500 만원\n",
      "+\n",
      "월생활자금  50만원 × 36개월 확정지급\n",
      "(총지급금액 1,800만원)\n",
      "※ 다만, 최초 계약은 가입 후 1년 미만 진단확정시\n",
      "     일시금 및 월생활자금의 50% 지급\n",
      "만기지급금만기 생존시\n",
      "-----------------------------------------------\n",
      "3. context to be fed into FM(e.g. Claude-v2)\n",
      "-----------------------------------------------\n",
      "(총지급금액 1,800만원)\n",
      "※ 다만, 최초 계약은 가입 후 1년 미만 진단확정시\n",
      "     일시금 및 월생활자금의 50% 지급\n",
      "말기간질환\n",
      "진단보험금15세 계약해당일 이후에\n",
      "말기간질환으로 진단확정시\n",
      "※ 다만, 최초 1회의 진단확정에 한함\n",
      "※ 갱신계약의 경우 [말기간질환 보장계약]\n",
      "     으로 갱신된 경우에 한함일시금  1,500 만원\n",
      "+\n",
      "-----------------------------------------------\n",
      "4. context to be fed into FM(e.g. Claude-v2)\n",
      "-----------------------------------------------\n",
      "※ 다만, 최초 1회의 진단확정에 한함\n",
      "※ 갱신계약의 경우 [급성심근경색증\n",
      "     보장계약]으로 갱신된 경우에 한함일시금  1,500 만원\n",
      "+\n",
      "월생활자금  50만원 × 36개월 확정지급\n",
      "(총지급금액 1,800만원)\n",
      "※ 다만, 최초 계약은 가입 후 1년 미만 진단확정시\n",
      "     일시금 및 월생활자금의 50% 지급\n",
      "말기신부전증\n",
      "-----------------------------------------------\n",
      "5. context to be fed into FM(e.g. Claude-v2)\n",
      "-----------------------------------------------\n",
      "으로 갱신된 경우에 한함일시금  1,500 만원\n",
      "+\n",
      "월생활자금  50만원 × 36개월 확정지급\n",
      "(총지급금액 1,800만원)\n",
      "※ 다만, 최초 계약은 가입 후 1년 미만 진단확정시\n",
      "     일시금 및 월생활자금의 50% 지급\n",
      "말기폐질환\n",
      "진단보험금15세 계약해당일 이후에\n",
      "말기폐질환으로 진단확정시\n",
      "※ 다만, 최초 1회의 진단확정에 한함\n",
      "-----------------------------------------------\n",
      "6. context to be fed into FM(e.g. Claude-v2)\n",
      "-----------------------------------------------\n",
      "말기신부전증, 말기간질환, 말기폐질환) 진단시 주계약 보험료 납입기간 동안\n",
      "차회 이후의 보험료를 내지 않고도 보장을 지속적으로 받을 수 있습니다.\n",
      "※ 보험료 납입이 면제되었다 하더라도 갱신되는 경우 갱신계약의 보험료는 납입해야 함\n",
      "아플 때 힘이 되도록 특정산정특례대상보장\n",
      "특약가입시 중증질환자[뇌혈관 및 심장질환] · 희귀질환자 산정특례대상으로\n",
      "-----------------------------------------------\n",
      "7. context to be fed into FM(e.g. Claude-v2)\n",
      "-----------------------------------------------\n",
      "225대질병 집중보장\n",
      "뇌출혈, 급성심근경색증, 말기신부전증, 말기간질환, 말기폐질환 진단시\n",
      "안심하고 치료받을 수 있도록 일시금과 매월 생활자금(3년)을 보장해 드립니다.\n",
      "맞춤형 종합보장\n",
      "생활보장특약 3개플랜(암/간병/상해플랜) 외 각종 입원 및 수술보장 등\n",
      "특약을 자유롭게 선택하여 라이프스타일에 맞게 연령에 맞게\n",
      "맞춤형 보장이 가능합니다.\n",
      "-----------------------------------------------\n",
      "8. context to be fed into FM(e.g. Claude-v2)\n",
      "-----------------------------------------------\n",
      "일시금 및 월생활자금의 50% 지급\n",
      "말기신부전증\n",
      "진단보험금말기신부전증으로 진단확정시\n",
      "※ 다만, 최초 1회의 진단확정에 한함\n",
      "※ 갱신계약의 경우 [말기신부전증 보장계약]\n",
      "     으로 갱신된 경우에 한함일시금  1,500 만원\n",
      "+\n",
      "월생활자금  50만원 × 36개월 확정지급\n",
      "(총지급금액 1,800만원)\n",
      "※ 다만, 최초 계약은 가입 후 1년 미만 진단확정시\n",
      "-----------------------------------------------\n",
      "9. context to be fed into FM(e.g. Claude-v2)\n",
      "-----------------------------------------------\n",
      "계약], [말기폐질환 보장계약] 각각의 보험료를 합한 경우의 예시입니다.주계약 보험료   기준 : 남자 40세, 월납, 주계약 보험가입금액 1,500만원, 20년 만기, 전기납\n",
      "(단위 : 원)\n",
      "보험료 납입면제 관련 사항(예시)\n",
      "-----------------------------------------------\n",
      "10. context to be fed into FM(e.g. Claude-v2)\n",
      "-----------------------------------------------\n",
      "갱신된 경우에 한함일시금  1,500 만원\n",
      "+\n",
      "월생활자금  50만원 × 36개월 확정지급\n",
      "(총지급금액 1,800만원)\n",
      "※ 다만, 최초 계약은 가입 후 1년 미만 진단확정시\n",
      "     일시금 및 월생활자금의 50% 지급\n",
      "급성심근경색증\n",
      "진단보험금급성심근경색증으로 진단확정시\n",
      "※ 다만, 최초 1회의 진단확정에 한함\n",
      "※ 갱신계약의 경우 [급성심근경색증\n"
     ]
    }
   ],
   "source": [
    "def show_context_used(context_list):\n",
    "    for idx, context in enumerate(context_list):\n",
    "        print(\"-----------------------------------------------\")                \n",
    "        print(f\"{idx+1}. context to be fed into FM(e.g. Claude-v2)\")\n",
    "        print(\"-----------------------------------------------\")        \n",
    "        print_ww(context.page_content)        \n",
    "        \n",
    "show_context_used(result['source_documents'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.결론\n",
    "RAG(검색 증강 생성)에 관한 이 모듈을 완료한 것을 축하합니다! 이는 대규모 언어 모델의 성능과 검색 방법의 정확성을 결합하는 중요한 기술입니다. 검색된 관련 사례로 생성을 강화함으로써 우리가 받은 응답은 더욱 일관되고 근거가 있게 됩니다. 당신은 이 혁신적인 접근 방식을 배우는 것에 자부심을 느껴야 합니다. 나는 당신이 얻은 지식이 창의적이고 매력적인 언어 생성 시스템을 구축하는 데 매우 유용할 것이라고 확신합니다. 잘하셨어요!\n",
    "\n",
    "위의 RAG 기반 질문 응답 구현에서 우리는 다음 개념과 Amazon Bedrock 및 LangChain 통합을 사용하여 이를 구현하는 방법을 탐색했습니다.\n",
    "\n",
    "- 문서 로드 및 임베딩 생성을 통해 벡터 저장소 생성\n",
    "- 질문에 대한 문서 검색\n",
    "- LLM에 대한 입력으로 사용되는 프롬프트 준비\n",
    "- 인간 친화적인 방식으로 답변 제시\n",
    "\n",
    "### 요약\n",
    "- 다양한 벡터 스토어로 실험해 보세요.\n",
    "- Amazon Bedrock에서 사용 가능한 다양한 모델을 활용하여 대체 출력을 확인하세요.\n",
    "- 임베딩 및 문서 청크의 영구 저장과 같은 옵션 탐색\n",
    "- 엔터프라이즈 데이터 저장소와의 통합\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
